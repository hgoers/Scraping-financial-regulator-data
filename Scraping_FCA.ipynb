{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-04\n",
      "00:09:00.938899\n"
     ]
    }
   ],
   "source": [
    "# Start time ---\n",
    "day_time = print(datetime.datetime.now().date())\n",
    "start_time = print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove the html code etc. from content \n",
    "def clean_up(text):\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub('\\\\n', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "titles = []\n",
    "pub_dates = []\n",
    "types = []\n",
    "summaries = []\n",
    "content = []\n",
    "\n",
    "# Get reference to pages on website\n",
    "pages = [str(i) for i in range(0,76)]\n",
    "\n",
    "# Set year\n",
    "year = 2017\n",
    "year = str(year)\n",
    "\n",
    "#For every page\n",
    "for page in pages:\n",
    "        \n",
    "    # Make a get request\n",
    "    response = get('https://www.fca.org.uk/news/search-results?start=' + page + '1&year=f.Published%20Year%7Cyear%3D' + year + '&sort_by=dmetaZ')\n",
    "\n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        warn('Request:{}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Select all containers from page\n",
    "    article_containers = soup.find_all('li', class_='search-item')\n",
    "\n",
    "    # Loop through all containers\n",
    "    i = 0\n",
    "    for i in range(0, len(article_containers)):\n",
    "\n",
    "        # Titles\n",
    "        title = article_containers[i].a.text\n",
    "        if title is None:\n",
    "            titles.append('NaN')\n",
    "        else: \n",
    "            titles.append(clean_up(title))\n",
    "            \n",
    "        # Pub date\n",
    "        pub_date = article_containers[i].find('span', class_='meta-item published-date').text\n",
    "        if pub_date is None:\n",
    "            pub_dates.append('NaN')\n",
    "        else:\n",
    "            pub_dates.append(clean_up(pub_date))\n",
    "                    \n",
    "        # Type\n",
    "        type_ = article_containers[0].find('span', class_='meta-item type').text\n",
    "        if type_ is None: \n",
    "            types.append('NaN')\n",
    "        else:\n",
    "            types.append(clean_up(type_))\n",
    "\n",
    "        # Summary\n",
    "        summary = article_containers[i].find('div', class_='search-item__body').text\n",
    "        if summary is None:\n",
    "            summaries.append('NaN')\n",
    "        else: \n",
    "            summaries.append(clean_up(summary))\n",
    "        \n",
    "        # Content\n",
    "        link = article_containers[i].find('a')['href']\n",
    "        \n",
    "        # Make a get request\n",
    "        response_link = get(link)\n",
    "        \n",
    "        # Throw a warning for non-200 status codes\n",
    "        if response.status_code != 200:\n",
    "            warn('Request:{}; Status code: {}'.format(requests, response_link.status_code))\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(response_link.text, 'html.parser')\n",
    "        \n",
    "        # Select all containers from page\n",
    "        link_container = str(soup.find_all('div', class_='field-item'))\n",
    "        if link_container is None:\n",
    "            content.append('NaN')\n",
    "        else: \n",
    "            content.append(clean_up(link_container))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe summary of information\n",
    "df = pd.DataFrame({'title': titles, \n",
    "                   'pub_date': pub_dates,\n",
    "                   'type': types,\n",
    "                   'summary': summaries,\n",
    "                   'content': content\n",
    "                  }\n",
    "                 )\n",
    "\n",
    "# Write dataframe to CSV\n",
    "df.to_csv(r'C:\\Users\\Harriet Goers\\Desktop\\UNSW\\FCA_data_' + year + '.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:33:57.559044\n"
     ]
    }
   ],
   "source": [
    "# End time --- \n",
    "endTime = print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
